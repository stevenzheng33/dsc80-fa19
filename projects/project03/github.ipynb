{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub WebHook Events\n",
    "\n",
    "This project uses webhook events from GitHub published by **GH Archive**. These events include most activities on public repositories like commits, pull-requests, forks, issues, comments, and more. Interesting questions to consider include:\n",
    "\n",
    "- Do people use GitHub differently in their personal life than in their jobs? Can you see a difference between weekday use and weekend use? (e.g. in the programming language used, whether users leave commit messages, whether a repository belongs to an organization).\n",
    "- \"Push Events\" correspond to users pushing their commits to GitHub. What do users' commits look like? How many commits to people make per push event? What do people write in their commit messages (do text analysis)? What does the usual vs. unusual commits look like?\n",
    "- Investigate forking repositories:\n",
    "> A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. Most commonly, forks are used to either propose changes to someone else's project or to use someone else's project as a starting point for your own idea.\n",
    "\n",
    "    - A forked repository is often a sign of a succesful repository, as users fork a repository to make their own customization or development. When do users fork repositories? What are the languages of the repositories being forked?\n",
    "- Investigate what programming language are used in projects. Are the languages in weekday vs weekend projects different? Are projects associated to an organization written in different languages to those which aren't? (In this dataset, the language is only available for \"fork\" events. However, you can follow URLs in this dataset to the GitHub API to find the language of any repository.\n",
    "\n",
    "\n",
    "### Getting the Data\n",
    "\n",
    "This project is unusual in that the process of getting the data is more involved and is more about the *collection*. As such, if your data collections is more involved and clever, the data analysis portion can be a little lighter.\n",
    "\n",
    "The primary data is explained on the [GH Archive](https://www.gharchive.org/) homepage. To get data for a single hour, in a terminal, type:\n",
    "```\n",
    "mkdir github_data\n",
    "cd github_data\n",
    "wget https://data.gharchive.org/2015-01-01-15.json.gz\n",
    "gunzip *\n",
    "cd ..\n",
    "```\n",
    "\n",
    "The website linked to has commands for pulling more data in one command. One day of data (24 hours) measures about 500MB in size. **You should do analyses on at least a few days of data for this project**. Once unzipped, the json file contains one valid json-object per line; the json is nested many-levels deep. Use [pandas.io.json.json_normalize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.json_normalize.html) to parse the lines on an hour or two of data. There will be approximately 600 columns. \n",
    "\n",
    "The following pattern should be useful:\n",
    "\n",
    "```\n",
    "filepath = '2015-01-01-15.json'\n",
    "L = [json.loads(x) for x in open('github_data/%s' % filepath)]\n",
    "df = pd.io.json.json_normalize(L)\n",
    "```\n",
    "\n",
    "Pick 15-25 interesting fields (see below) and write a function that takes in a file (representing a single hour of github activity) and returns a dataframe with only those fields. Then iterate through the files to concatenate all the data into a single dataframe.\n",
    "\n",
    "A few interesting fields of note are (the 'dot' notation refers to the nested dictionary keys):\n",
    "* type\n",
    "    - The type of event (push-event, pull-request, fork, etc).\n",
    "* actor.login\n",
    "    - The GitHub user-id of the user\n",
    "* actor.url\n",
    "    - The url for more information about the user (e.g. Location, number of followers/folowees, number of repos).\n",
    "* repo.name\n",
    "    - The name of the repository.\n",
    "* repo.url\n",
    "    - The url for more information about the repo (e.g. programming language, forks, watchers)\n",
    "* created_at\n",
    "    - When the event occurred.\n",
    "* org.login\n",
    "    - The name of the organization to which the repository belongs.\n",
    "* payload.commits\n",
    "    - The commits for the push-event (it is a list that must be separately parsed)\n",
    "* payload.forkee.*\n",
    "    - Information about fork events.\n",
    "* payload.pull_request.*\n",
    "    - Information about pull requests\n",
    "\n",
    "\n",
    "In particular, every field that ends in `url` refers to an endpoint of the *GitHub API* that contains more information about that event. You are encouraged to write requests to grab data from those files as well!\n",
    "\n",
    "\n",
    "### Cleaning and EDA\n",
    "\n",
    "- Create a single DataFrame with the relevant fields contain the events over the time range chosen.\n",
    "- Understand the data in ways relevant to your question using univariate and bivariate analysis of the data as well as aggregations.\n",
    "\n",
    "\n",
    "### Assessment of Missingness\n",
    "\n",
    "\n",
    "Many columns which have `NaN` values are missing-by-design. For example, the `payload.forkee.*` fields are only present for events of type `fork`. You should assess the missing of columns that are **not** missing-by-design. Either in the GH-Archive data, or in supplementary data pulled from the GitHub API.\n",
    "\n",
    "### Hypothesis Test / Permutation Test\n",
    "Find a hypothesis test or permutation test to perform. You can use the questions at the top of the notebook for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "### Introduction\n",
    "TODO\n",
    "\n",
    "### Cleaning and EDA\n",
    "TODO\n",
    "\n",
    "### Assessment of Missingness\n",
    "TODO\n",
    "\n",
    "### Hypothesis Test\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test / Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
