{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unstructured Text\n",
    "\n",
    "* Lecture Credits (First half):\n",
    "    - [Berkeley: DS100](https://docs.google.com/presentation/d/1ECr_XrDJXaLK-eGwWlydLjJu-3xwpzFcV4fGMSfwKwU/edit?usp=sharing)\n",
    "    - [Princeton: COS226](http://www.cs.princeton.edu/courses/archive/spring17/cos226/lectures/54RegularExpressions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Canonicalization\n",
    "* Regex\n",
    "* Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Joining on text\n",
    "\n",
    "* Will these join? What are the problems?\n",
    "* How would you clean the dataframes to join them?\n",
    "<img src=\"imgs/image_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Joining on text: cleaning the join key\n",
    "* Upper vs lower case\n",
    "* Words \"county\" and \"parish\" don't add information in these datasets\n",
    "* Take care of common variants of words\n",
    "* Take care of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state = pd.read_csv(\"data/county_and_state.csv\")\n",
    "county_and_pop = pd.read_csv(\"data/county_and_pop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(county_and_state)\n",
    "display(county_and_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive join\n",
    "county_and_state.merge(county_and_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_county(county):\n",
    "    return (county\n",
    "            .lower()\n",
    "            .strip()\n",
    "            .replace(' ', '')\n",
    "            .replace('county', '')\n",
    "            .replace('parish', '')\n",
    "            .replace('&', 'and')\n",
    "            .replace('.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop['County'] = county_and_pop['County'].apply(clean_county)\n",
    "county_and_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state['County'] = county_and_state['County'].apply(clean_county).to_frame()\n",
    "county_and_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state.merge(county_and_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonicalization\n",
    "* Create a sequence of steps that transforms both columns into a single form.\n",
    "\n",
    "<img src=\"imgs/image_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonicalization\n",
    "\n",
    "Replace each string with a unique representation.\n",
    "- Used string methods\n",
    "- Very brittle procedure; may only work for X% of the data.\n",
    "- Hard to verify correctness.\n",
    "- Also *parse* data using a data model if given the choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question: limitations of string methods \n",
    "\n",
    "* Suppose we want to extract the date and time from the following string:\n",
    "```\n",
    "170.242.51.168 - - [14/Mar/2018:12:09:20 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585\n",
    "```\n",
    "* How would you do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of string methods \n",
    "\n",
    "```\n",
    "170.242.51.168 - - [14/Mar/2018:12:09:20 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585\n",
    "```\n",
    "Steps:\n",
    "1. Get string between `[` and `]`\n",
    "2. Parse date as day/month/year:hour:min:sec without the timezone\n",
    "    - in `strftime` format: `%d/%b/%y:%H:%M:%S`\n",
    "    \n",
    "What could go wrong with this technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''170.242.51.168 - - [14/Mar/2018:12:09:20 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585'''\n",
    "\n",
    "dt = s[s.find('[') + 1:s.find(']') - 6]\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day, month, year_time = dt.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, hour, minute, second =  year_time.split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, month, day, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_datetime(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better: if date format changes, will throw a descriptive error! (good!)\n",
    "pd.to_datetime(dt, format='%d/%b/%Y:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular Expressions (`regexp`)\n",
    "\n",
    "* Fast, compact way of matching patterns in text\n",
    "* Python library: `import re`\n",
    "* Advantages: powerful; capable of matching very complex patterns.\n",
    "* Disadvantages: \n",
    "    - It's still text processing, so brittle and likely to break.\n",
    "    - Hard to understand: \"write-only\" language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular expression solution to log parsing\n",
    "* Regex pattern uses the same logic as log parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = '\\[(.+)\\/(.+)\\/(.+):(.+):(.+):(.+) .+\\]'\n",
    "re.findall(pat, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular Expressions: the more specific, the better!\n",
    "* Be as specific in your pattern matching as possible\n",
    "    - Easier to validate the extracted text\n",
    "    - Easier for error handling (understanding what went wrong).\n",
    "    \n",
    "* A better date extraction pattern uses:\n",
    "```\n",
    "'\\[([0-9]{2}\\/[A-Z]{1}[a-z]{2}\\/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2} -[0-9]{4})\\]'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '170.242.51.168 - - [14/Mar/2018:12:09:20 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '170.242.51.168 - - [14/Mar/2018 12:09:20 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = '\\[(.+\\/.+\\/.+:.+:.+:.+ .+)\\]'\n",
    "re.findall(pat1, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2 = '\\[([0-9]{2}\\/[A-Z]{1}[a-z]{2}\\/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2} -[0-9]{4})\\]'\n",
    "re.findall(pat2, s)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regexp Expressions\n",
    "* Parsing the expression:\n",
    "```\n",
    "'\\[([0-9]{2}\\/[A-Z]{1}[a-z]{2}\\/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2} -[0-9]{4})\\]'\n",
    "```\n",
    "\n",
    "* `[0-9]{2}` matches any 2-digit number.\n",
    "* `[A-Z]{1}` matches any single occurrence of any upper-case letter.\n",
    "* `[a-z]{2}` matches any 2 consecutive occurrences of lower-case letters.\n",
    "* Certain special characters (`[`, `]`, `/`) need to be escaped with `\\`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is pattern 2 better?\n",
    "t = '[adr/jduy/wffsdffs:r4s4:4wsgdfd:asdf 7]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(pat, t)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy to check if something mis-parsed: empty list!\n",
    "re.findall(pat2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular Expressions: Applications\n",
    "\n",
    "* Source Code: IDE syntax highlighting\n",
    "* Google Code Search\n",
    "* Scanning for viruses\n",
    "* Validating text for data-entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You may already know regular expressions\n",
    "* Google supports wildcard search `*` and union `|`\n",
    "![google](imgs/regex_google.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regular Expression Syntax\n",
    "\n",
    "* The four 'building blocks' for all regular expressions:\n",
    "    - \"order\" specifies \"order of operations\"\n",
    "\n",
    "<img src=\"imgs/image_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of each\n",
    "re.search('AB*A', 'ABBBBBBA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('AB*A', 'AB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion\n",
    "\n",
    "1. Give a regular expression that matches \"moon\", \"moooon\", etc. \n",
    "    - Should match any **even** positive number of 'o'.\n",
    "2. Give a regex that matches muun, muuuun, moon, moooon, etc. \n",
    "    - Should match any **even** positive number of 'u' and 'o'.\n",
    "\n",
    "<img src=\"imgs/image_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expanded Regexp Syntax\n",
    "\n",
    "* The following operations offer convenience for expressive matching.\n",
    "\n",
    "<img src=\"imgs/image_4.png\">\n",
    "\n",
    "* Ex. `[A-E]+` is just shorthand for `(A|B|C|D|E)(A|B|C|D|E)*`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Regular Expression Examples\n",
    "\n",
    "\n",
    "<img src=\"imgs/image_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Discussion Question\n",
    "\n",
    "* Give a regular expression for any lowercase string that has a repeated vowel (i.e. noon, peel, festoon, looop, etc).\n",
    "* Give a regular expression for any string that contains both a lowercase letter and a number, in any order.\n",
    "\n",
    "<img src=\"imgs/image_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Even More Regular Expressions\n",
    "\n",
    "<img src=\"imgs/image_7.png\">\n",
    "\n",
    "* Anchors (`^` and `$`) will be useful on HW).\n",
    "* Non-greedy qualifier is useful, but won't be used in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of Regular Expressions\n",
    "\n",
    "Writing regular expressions is like writing a program.\n",
    "* Need to know the syntax well.\n",
    "* Can be easier to write than to read.\n",
    "* Can be difficult to debug.\n",
    "\n",
    "Regular expressions terrible at certain types of problems. Examples:\n",
    "* Anything involving counting (same number of instances of a and b).\n",
    "* Anything involving complex structure (palindromes).\n",
    "* Parsing highly complex text structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Email address validation using regexp\n",
    "\n",
    "<img src=\"imgs/image_8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regexp for data validation\n",
    "* See [postmortem](http://stackstatus.net/post/147710624694/outage-postmortem-july-20-2016)\n",
    "![so_regexp](imgs/so_regexp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More regexp syntax\n",
    "\n",
    "For more regexp info, see:\n",
    "* [DS100 Textbook](https://www.textbook.ds100.org/ch/08/text_regex.html)\n",
    "* [Python Documentation](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "<img src=\"imgs/image_9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python `re` library functions\n",
    "* `re.search`:\n",
    "    - `m = re.search(r, s); m.groups`\n",
    "* `re.findall`\n",
    "* `re.sub`\n",
    "\n",
    "Also in Pandas: `Series.str.sub`, `Series.str.extract`, `Series.str.contains`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regexp Groups (Briefly)\n",
    "* Use `(` regex `)` to *extract* text from a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pattern = '([0-9]{3}\\.[0-9]{3}\\.[0-9]{2}\\.[0-9]{3})'\n",
    "dt_pattern = '\\[(.*)\\]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(ip_pattern, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(dt_pattern, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = '%s|%s' %(ip_pattern, dt_pattern)\n",
    "pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now have *two* possible group matches!\n",
    "re.findall(pat, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: San Diego City Employee Salaries\n",
    "* Recall Lecture 01: Do men and women make similar salaries?\n",
    "* Follow-up question: Do men and women make similar salaries among those with similar jobs?\n",
    "    - How to determine \"similar jobs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('https://transcal.s3.amazonaws.com/public/export/san-diego-2017.csv')\n",
    "jobtitles = salaries['Job Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.value_counts().iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.shape[0], jobtitles.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.value_counts().iloc[:100].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cleaning job titles: assessment\n",
    "* Can we canonicalize job titles?\n",
    "* Are they self-consistent?\n",
    "    - Punctuation? Capitalization? Abbreviations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run / rerun\n",
    "jobtitles.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cleaning job titles: step 1\n",
    "* Is every first letter capitalized?\n",
    "* What punctuation exists? should it be cleaned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalization\n",
    "jobtitles[(\n",
    "    jobtitles.str.contains(' [a-z]+ ') | \n",
    "    jobtitles.apply(lambda x:x[0].islower())\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation: replace with space?\n",
    "jobtitles[jobtitles.str.contains('[^A-Za-z0-9 ]')].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cleaning job titles: step 1\n",
    "* Remove: to, the, for\n",
    "* Replace: non-alphanumeric with space\n",
    "* Replace multiple spaces with one space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles = (\n",
    "    jobtitles\n",
    "    .str.replace(' to| the| for', '')  # include the spaces! (why?)\n",
    "    .str.replace(\"[^A-Za-z0-9' ]\", ' ')\n",
    "    .str.replace(\"'\", '')\n",
    "    .str.replace(' +', ' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cleaning job titles: abbreviations \n",
    "\n",
    "* Which job titles are inconsistently described?\n",
    "    - Librarian? Engineer? Director?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles[jobtitles.str.contains('Libr')].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles[jobtitles.str.contains('Eng')].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles[jobtitles.str.contains('Dir')].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The limits of canonicalization\n",
    "* How do we find which common words to map a job title to?\n",
    "    - E.g. pattern matching on 'Eng' or 'Libr'\n",
    "* What about other titles? (there are too many!)\n",
    "* Adjectives vs Nouns have different meanings\n",
    "    - Junior/Senior/Director vs Police/Fire/Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The limits of canonicalization\n",
    "\n",
    "Naive procedure: \n",
    "1. Compute most common words\n",
    "2. Select \"relevant\" words using domain expertise\n",
    "3. Check if a given word is contained in a given job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute counts of all words (together)\n",
    "bow = jobtitles.str.split().sum()\n",
    "words = pd.Series(bow).value_counts()\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is a given word in a job-title? count number of occurances of each\n",
    "jobtypes = pd.DataFrame([], index=salaries.index)\n",
    "for word in words.index:\n",
    "    re_pat = '\\\\b{word}\\\\b'.format(word=word)\n",
    "    jobtypes[word] = jobtitles.str.count(re_pat).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtypes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many columns? (curse of dimensionality)\n",
    "len(jobtypes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtypes.iloc[:,:20].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about those with sum = 0? sum > 1?\n",
    "jobtypes.iloc[:,:20].sum(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the closest job titles to 'Asst Fire Chief'\n",
    "\n",
    "\n",
    "* Idea: which other job titles share \"the most\" words in common?\n",
    "* Implementation: use the 'word vectors' in `jobtypes` to count up matching words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the closest job titles to:\n",
    "jobidx = 109\n",
    "job = jobtitles.iloc[jobidx]\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job1 = jobtitles.iloc[0]\n",
    "job1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors side-by-side\n",
    "pd.concat([jobtypes.iloc[0], jobtypes.iloc[109]], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = pd.concat([jobtypes.iloc[0], jobtypes.iloc[109]], axis=1)\n",
    "\n",
    "(cnts.iloc[:,0] * cnts.iloc[:,1]).head(10).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the matches\n",
    "np.sum(cnts.iloc[:,0] * cnts.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution attempt 1: bag of words\n",
    "1. Create a list of all words appearing among *all* text ('bag of words')\n",
    "2. Create a vector, indexed by the distinct words, with counts of the words in that entry.\n",
    "3. Two text entries are similar if the sum of their matches is large.\n",
    "    - The sum of their matches is the same as the dot-product of the vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion Question\n",
    "\n",
    "Given the list of sentences below:\n",
    "1. What is the index for the word vectors of the sentences?\n",
    "2. How close are the word-vectors of the first and second sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'the fox and the moon',\n",
    "    'the cow and the moon',\n",
    "    'the cow and the spoon'\n",
    "]\n",
    "pd.Series(sentences).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.Series(sentences)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.Series(sentences.str.split().sum()).value_counts()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvecs = pd.DataFrame([], index=sentences.index)\n",
    "for word in words.index:\n",
    "    re_pat = '\\\\b%s\\\\b' % word\n",
    "    wordvecs[word] = sentences.str.count(re_pat).astype(int)\n",
    "    \n",
    "wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(wordvecs.iloc[0] * wordvecs.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of Words: Salaries\n",
    "* Compute the sum of matches among all word vectors and 'Asst Fire Chief'\n",
    "* Take the job that is the closest match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.iloc[jobidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobvec = jobtypes.iloc[jobidx]\n",
    "jobvec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product with 'Asst Fire Chief' and *all* other titles\n",
    "matches = jobtypes.apply(lambda ser: np.dot(jobvec, ser), axis=1)\n",
    "matches.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitles.loc[matches.sort_values(ascending=False).index].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Bag of Words\n",
    "\n",
    "* Create an index out of *all* distinct words \n",
    "    - The basis for the vector space of words.\n",
    "* Create vectors for each text entry by computing the counts of words in the entry.\n",
    "* The dot product between two vectors is proportional to their 'similarity':\n",
    "    - This defines the **cosine similarity** between vectors via: $$dist(v, w) = 1 - \\cos(\\theta) = 1 - \\frac{v \\cdot w}{|v||w|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion: Bag of Words\n",
    "* Bag of words *embeds words into a vector space*\n",
    "* Can then use clustering (e.g. k-means) to group like words (e.g. into 'job-types')\n",
    "    - Unfortunately, many clustering techniques don't work well in high dimensions.\n",
    "* Downside: treats all words as *equally important*.\n",
    "    - \"Asst Fire Chief\" vs \"Asst Chief Oper Ofcr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "Term Frequency / Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency, Inverse Document Frequency\n",
    "\n",
    "How do we figure out which words are \"important\" in a document?\n",
    "\n",
    "1. The most common words often *don't* have much meaning!\n",
    "2. The very rare words are also less important!\n",
    "\n",
    "Goal: balance these two observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency, Inverse Document Frequency\n",
    "\n",
    "* The *term frequency* of a word $t$ in a document $d$, denoted ${\\rm tf}(t,d)$, is the likelihood of the term appearing in the document.\n",
    "   * Word that occurs often is important to document meaning.\n",
    "\n",
    "* The *document frequency* is how often the a words occurs in the entire set of documents.\n",
    "   * Common words that appear everywhere.\n",
    "\n",
    "\n",
    "* Question: what are the frequencies for a word \"the\"? (high/low?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about their ratio? Intuition\n",
    "\n",
    "The relevance of this word to the document.\n",
    "\n",
    "$$\\frac{{\\rm\\ TermFrequency}}{{\\rm DocumentFrequncy}}$$\n",
    "\n",
    "* `TF`: High, `DC`: High\n",
    "* `TF`: High, `DC`: Low\n",
    "* `TF`: Low, `DC`: High\n",
    "* `TF`: Low, `DC`: Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency, Inverse Document Frequency\n",
    "\n",
    "* The *term frequency* of a word $t$ in a document $d$, denoted ${\\rm tf}(t,d)$, is the likelihood of the term appearing in the document.\n",
    "* The *inverse document frequency* of a word $t$ in a set of documents $\\{d_i\\}$, denoted ${\\rm idf}(t,d)$ is: \n",
    "\n",
    "$$\\log(\\frac{{\\rm\\ total\\ number\\ of\\ documents}}{{\\rm number\\ of\\ documents\\ in\\ which\\ t\\ appears}})$$\n",
    "\n",
    "* The *tf-idf* of a term $t$ in document $d$ is given by the product: \n",
    "\n",
    "$${\\rm tfidf}(t,d) = {\\rm tf}(t,d) \\cdot {\\rm idf}(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the tf-idf of 'cow' in the second 'document'?\n",
    "sentences.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the term frequency of 'cow' in the second 'document'\n",
    "tf = sentences.iloc[1].count('cow') / (sentences.iloc[1].count(' ') + 1)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(len(sentences) / sentences.str.contains('cow').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF of all terms in all documents\n",
    "* What are the different reasons tf-idf can be zero?\n",
    "* When is it the largest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.Series(sentences.str.split().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame([], index=sentences.index)  # dataframe of documents\n",
    "for w in words.unique():\n",
    "    re_pat = '\\\\b%s\\\\b' % w\n",
    "    tf = sentences.str.count(re_pat) / (sentences.str.count(' ') + 1)\n",
    "    idf = np.log(len(sentences) / sentences.str.contains(re_pat).sum())\n",
    "    tfidf[w] = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: TF-IDF\n",
    "\n",
    "* Term Frequency, Inverse Document Frequency balances:\n",
    "    - how often a word appears in a document/sentence, with\n",
    "    - how often a word appears *across* documents.\n",
    "* For a given document, the word with the highest TF-IDF best summarizes that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: State of the Union Addresses\n",
    "\n",
    "* What are the important words for each address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu = open('data/stateoftheunion1790-2017.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sotu[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = sotu.split('\\n***\\n')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_struct(speech):\n",
    "    L = speech.strip().split('\\n', maxsplit=3)\n",
    "    L[3] = re.sub(\"[^A-Za-z' ]\", ' ', L[3]).lower()\n",
    "    return dict(zip(['speech', 'president', 'date', 'contents'], L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(map(extract_struct, speeches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.Series(df.contents.str.split().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame([], index=df.index)  # dataframe of documents\n",
    "tf_denom = (df.contents.str.count(' ') + 1)\n",
    "for w in words.value_counts().iloc[0:500].index:\n",
    "    # imperfect pattern match for speed\n",
    "    re_pat = ' %s ' % w\n",
    "    tf = df.contents.str.count(re_pat) / tf_denom\n",
    "    idf = np.log(len(df) / df.contents.str.contains(re_pat).sum())\n",
    "    tfidf[w] =  tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = tfidf.idxmax(axis=1)\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([summaries, df.president], axis=1).groupby('president').apply(lambda x:x[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
